_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
inception_v3_input (InputLay (None, 210, 160, 3)       0         
_________________________________________________________________
inception_v3 (Model)         (None, 5, 3, 2048)        21802784  
_________________________________________________________________
global_average_pooling2d_1 ( (None, 2048)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 6)                 12294     
=================================================================
Total params: 21,815,078
Trainable params: 21,768,352
Non-trainable params: 46,726
_________________________________________________________________

def drop_frames(input_tensor):
    return input_tensor[0,0]

for __ in range(8):
    for increase_confidence in range(5):
        window_length = 5
        memory_size = 2048
        batch_size = 256
        env = gym.make(ENV_NAME)

        input_model = Sequential()

        inception_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(210, 160, 3))

        input_model.add(inception_model)
        # add a global spatial average pooling layer
        x = input_model.output
        x = GlobalAveragePooling2D()(x)
        # let's add a fully-connected layer
        predictions = Dense(nb_actions, activation='linear',  trainable=False)(x)

        # this is the model we will train
        model = Model(inputs=input_model.input, outputs=predictions)


        episode_count = 16
        step_count = 512 * episode_count
        memory = SequentialMemory(limit=memory_size, window_length=window_length)
        policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=.75, value_min=0.075, value_test=0,
                                     nb_steps=episode_count * 512 / 2)

        dqn = DQNAgent(model=model, nb_actions=nb_actions, policy=policy, memory=memory,
               nb_steps_warmup=512, gamma=.925, target_model_update=1e-2,
               train_interval=1, batch_size=batch_size, delta_clip=1., enable_dueling_network=True, dueling_type="avg")

        dqn.compile(Adam(lr=1e-3), metrics=['mae'])

        dqn.fit(env, nb_steps=step_count, visualize=True, verbose=0, callbacks=[callbacks.TensorBoard(log_dir=run_path, write_graph=True, write_images=True)])


		self.EPISODE_LENGTH = 768
		self.ACTION_LENGTH = 12

		# This is the action space.
		self.action_dict = {
			0: "A",
			1: "Right",
			2: "Left",
			3: "B",
			4: ""
			# 3: "Down",
			# 4: "B"
		}

		# This is the state
		return current_vector_with_memory_from_ss_plus_last_action()

		def plusone_increase_minus_half_decrease_or_stationary():
			distance = self.get_distance()
			delta = distance - self.last_distance

			reward = -.5
			# The reward amounts
			if delta > 0:
				reward = 1
			elif delta < 0:
				reward = -.5

			self.last_distance = distance
			return reward

		def distance_traveled_between_frames_minus_for_nochange():
			distance = self.get_distance()
			delta = distance - self.last_distance

			self.last_distance = distance
			if delta == 0:
				delta = -0.1
			if delta > 30:
				delta = 0
			if delta < -1250:
				delta = 0
			return delta

		# The reward is:
		reward = increase_the_max_bounding_box()
